---
title: "SOC & tornadoes"
output: html_document
date: "2024-01-05"
editor_options: 
  chunk_output_type: console
---

Analysis of daily U.S. tornado frequency distributions following the work of https://rpubs.com/jelsner/DailyCounts using data from 2013 through 2022

# Preliminary analyses

Get the latest tornado reports from the SPC
```{r}
download.file(url = "http://www.spc.noaa.gov/wcm/data/1950-2022_actual_tornadoes.csv",
              destfile = here::here("data", "Tornadoes.csv"))
Torn.df <- readr::read_csv(file = here::here("data", "Tornadoes.csv"))
```

```{r}
Torn.df |>
  dplyr::filter(yr >= 2013) |>
  dplyr::group_by(yr) |>
  dplyr::summarize(nT = dplyr::n()) |>
  dplyr::summarize(Avg = mean(nT),
                   IQR = IQR(nT))
```

In the ten-year period 2013–2022 there was an average of 1156 tornadoes per year with an interquartile range of 277 tornadoes.

```{r}
YD.df <- Torn.df |>
  dplyr::filter(yr >= 2013) |>
  dplyr::group_by(date, yr) |>
  dplyr::summarize(nT = dplyr::n(), 
                   nVTor = sum(mag >= 4),
                   nNVTor = sum(mag < 4))
Y.df <- Torn.df |>
  dplyr::filter(yr >= 2013) |>
  dplyr::group_by(yr) |>
  dplyr::summarize(nT = dplyr::n())

YmaxD.df <- YD.df |>
  dplyr::group_by(yr) |>
  dplyr::summarize(nTdays = dplyr::n(),
                   maxTor = max(nT))

YmaxD.df <- YmaxD.df |>
  merge(Y.df, by = "yr") |>
  dplyr::mutate(frac = maxTor/nT)

YmaxD.df |>
  dplyr::summarize(minFrac = min(frac),
                   maxFrac = max(frac),
                   minYr = yr[which.min(frac)],
                   maxYr = yr[which.max(frac)])

min(YmaxD.df$frac) * 100
maxFrac = max(YmaxD3.df$frac) * 100
minYr = YmaxD3.df$Year[which.min(YmaxD3.df$frac)]
maxYr = YmaxD3.df$Year[which.max(YmaxD3.df$frac)]
```

The percentage of the annual count occurring on the day of the year with the most tornadoes ranged from a low of 3.6\% in 2018 to a high of 9.5\% in 2021.

```{r}
Torn.df |>
  dplyr::filter(yr >= 2013) |>
  dplyr::summarize(nT = dplyr::n(),
                   nVTor = sum(mag >= 4),
                   perc = nVTor / nT * 100)
```

What days had the most tornadoes and how many?
```{r}
YD.df |>
  dplyr::group_by(yr) |>
  dplyr::summarise(Date = date[which.max(nT)],
                   nT = max(nT))
```

Moreover, tornadoes are rated on a categorical damage scale ranging from weak EF0 to violent EF4 and EF5. Of the 11,555 U.S. tornadoes in the database over the 10-year period only 0.34\% were rated as violent.

```{r}
( FreqByMag.df <- Torn.df |>
  dplyr::filter(yr >= 2013 & mag >= 1) |>
  dplyr::group_by(mag) |>
  dplyr::summarise(nT = dplyr::n()) )
```

```{r}
( Total <- sum(FreqByMag.df$nT) )
FreqByMag.df$nT / Total
```


```{r}
FreqByMag.df$nT[1:4] / FreqByMag.df$nT[2:5]
```

There are 5283 tornado reports (rated EF1 or higher) over the period 2013–2022 inclusive. Table~\ref{TotalDist} gives the distribution of the reports by EF rating by count and by percentage of the total. It also gives the factor by which the frequency in the category exceeds the frequency in the next highest category.

# Frequency distribution of tornado days

```{r}
Torn2.df <- Torn.df |>
  dplyr::filter(yr >= 1994 & mag >= 1)

( allDays <- as.integer(as.Date("2022-12-31") - as.Date("2013-01-01")) )

YD2.df <- Torn2.df |>
    dplyr::group_by(date, yr) |>
    dplyr::summarise(nTor = dplyr::n(),
                     nVTor = sum(mag >= 4))

( torDays <- nrow(YD2.df) )
( per <- torDays/allDays * 100 )

```

28.3\% of all days during this 10-year period had at least one EF1 or more damaging tornado. This compares with 29\% of all days during the period 1994-2012

```{r}
( mTorNV <- round(mean(YD2.df$nTor[YD2.df$nVTor == 0])) )
( mTorV = round(mean(YD2.df$nTor[YD2.df$nVTor != 0])) )
```

The average number of tornadoes on tornado days without a violent tornado (EF4 or EF5) is 5 which compares to an average of 20 tornadoes on days with at least one violent tornado

```{r}
library(ggplot2)

p2a <- YD2.df |>
  dplyr::group_by(yr) |>
  dplyr::summarise(nTdays = dplyr::n(),
                   maxNtor = max(nTor)) |>
ggplot(mapping = aes(x = yr, y = nTdays)) +
  geom_col(fill = "grey") +
  scale_x_continuous(breaks = 2013:2022) +
  theme_bw() +
  labs(y = "Tornado days", x = "")

p2b <- YD2.df |>
  dplyr::group_by(yr) |>
  dplyr::summarise(nTdays = dplyr::n(),
                   maxNtor = max(nTor)) |>
ggplot(mapping = aes(x = yr, y = maxNtor)) +
  geom_col(fill = "grey") +
  scale_x_continuous(breaks = 2013:2022) +
  theme_bw() +
  labs(y = "Number of tornadoes\n on the day with the most", x = "")
  
library(patchwork)
p2a / p2b
```

```{r}
tab <- table(YD2.df$nTor)
df <- tab |>
  data.frame()
df$Size <- as.integer(names(tab))

( p3a <- ggplot(data = df,
              mapping = aes(x = Size, y = Freq)) + 
  scale_x_log10() + scale_y_log10() + 
  geom_point() + 
  labs(x = expression(paste("Size (Number of tornadoes ", d^{-1}, ")")), 
       y = "Frequency (Number of days)") +
  theme_bw() )
```

The VGAM package is no longer supported

```{r}
library(VGAM)
zipf.law <- vglm(formula = Size ~ 1, 
     data = df,
     weights = Freq,
     family = zipf(lshape = "loglink"))

summary(zipf.law)
exp(.4785)
```

# poweRlaw package

```{r}
library(poweRlaw)
citation("poweRlaw")

m_pl <- displ$new(df$Freq)
est <- estimate_xmin(m_pl)
m_pl$setXmin(est)

p.df <- plot(m_pl)
lines(m_pl, col = 2)
```
x: number of tornadoes
y: percentage of tornado days with at least that many tornadoes

```{r}
parallel::detectCores()

bs <- bootstrap(m_pl, no_of_sims = 1000, threads = 10)

hist(bs$bootstraps[, 2], breaks = "fd")
hist(bs$bootstraps[, 3], breaks = "fd")
plot(jitter(bs$bootstraps[, 2], factor = 1.2), bs$bootstraps[, 3])
```

Do we have a power law?
```{r}
bs_p <- bootstrap_p(m_pl, no_of_sims = 1000, threads = 10)
bs_p$p
```

With this high p-value we cannot rule out a power-law model

# All tornadoes

```{r}
Days.df <- readr::read_csv(file = here::here("data", "Tornadoes.csv"),
                             show_col_types = FALSE) |>
  dplyr::filter(mag >= 2, yr > 1989) |>
  dplyr::group_by(date, yr) |>
  dplyr::reframe(nTor = dplyr::n())

Days.df |>
  dplyr::group_by(yr) |>
  dplyr::reframe(nDays = dplyr::n(),
                 nTor = sum(nTor)) |>
  ggplot(mapping = aes(x = yr, y = nDays)) +
  geom_point() +
  geom_smooth()
```

```{r}
tab <- table(Days.df$nTor)
df <- tab |>
  data.frame()
df$Size <- as.integer(names(tab))

ggplot(data = df,
       mapping = aes(x = Size, y = Freq)) + 
  scale_x_log10() + scale_y_log10() + 
  geom_point() + 
  labs(x = expression(paste("Size (Number of tornadoes ", d^{-1}, ")")), 
       y = "Frequency (Number of days)") +
  theme_bw() 
```

```{r}
Freq <- sort(df$Freq, decreasing = TRUE)
m_pl <- displ$new(Freq)
est <- estimate_xmin(m_pl)
m_pl$setXmin(est)

m_pl$pars
m_pl$xmin

plot(m_pl)
```

```{r}
bs <- bootstrap(m_pl, no_of_sims = 1000, threads = 10)

hist(bs$bootstraps[, 2], breaks = "fd")
hist(bs$bootstraps[, 3], breaks = "fd")
plot(jitter(bs$bootstraps[, 2], factor = 1.2), bs$bootstraps[, 3])
```

Do we have a power law?
```{r}
bs_p <- bootstrap_p(m_pl, no_of_sims = 1000, threads = 10)
bs_p$p
```

Assuming a power law, how does the exponent change by year?

```{r}
w <- NULL
xmin <- NULL
for(i in 1950:2022){
  print(i)
  Days.df <- readr::read_csv(file = here::here("data", "Tornadoes.csv"),
                             show_col_types = FALSE) |>
  dplyr::filter(mag >= 2, yr == i) |>
  dplyr::group_by(date, yr) |>
  dplyr::reframe(nTor = dplyr::n())
  
df <- table(Days.df$nTor) |>
  data.frame()

Freq <- sort(df$Freq, decreasing = TRUE)
m_pl <- displ$new(Freq)
est <- estimate_xmin(m_pl)
#est$xmin = 1
m_pl$setXmin(est)

xmin <- c(xmin, m_pl$xmin)
w <- c(w, m_pl$pars)
}

w.df <- data.frame(Year = 1950:2022, w)

ggplot(data = w.df,
       mapping = aes(x = Year, y = w)) +
#  scale_y_continuous(limits = c(1, 2)) +
  scale_x_continuous(breaks = seq(1950, 2020, by = 10)) +
  geom_point() +
  geom_quantile()

summary(lm(w~Year, data = w.df))
```

Get the EF2+ tornadoes from Grazulis and check for daily count frequency scaling

Get the historical tornado data
```{r}
OldTorn.sf <- sf::st_read(dsn = here::here("data", "SignificantTornadoes"), 
                          layer = "SignificantTornadoes") |>
  dplyr::mutate(date = as.Date(date)) |>
  dplyr::arrange(date, time) |>
  dplyr::select(-county) |>
  dplyr::filter(mag %in% c("F2", "F3", "F4", "F5")) # note some transcription errors


OldDays.df <- OldTorn.sf |>
  sf::st_drop_geometry() |>
  dplyr::group_by(date) |>
  dplyr::reframe(nTor = dplyr::n(),
                 yr = lubridate::year(date))
```

```{r}
tab <- table(OldDays.df$nTor)
df <- tab |>
  data.frame()
df$Size <- as.integer(names(tab))

ggplot(data = df,
       mapping = aes(x = Size, y = Freq)) + 
  scale_x_log10() + scale_y_log10() + 
  geom_point() + 
  labs(x = expression(paste("Size (Number of tornadoes ", d^{-1}, ")")), 
       y = "Frequency (Number of days)") +
  theme_bw() 
```

```{r}
Freq <- sort(df$Freq, decreasing = TRUE)
m_pl <- displ$new(df$Freq)
est <- estimate_xmin(m_pl)
m_pl$setXmin(est)

m_pl$pars
m_pl$xmin

plot(m_pl)
```

```{r}
w <- NULL
xmin <- NULL
for(i in 1880:1989){
  print(i)
  OldTorn.sf <- sf::st_read(dsn = here::here("data", "SignificantTornadoes"), 
                          layer = "SignificantTornadoes",
                          quiet = TRUE) |>
  dplyr::filter(Year == i) |>
  dplyr::mutate(date = as.Date(date)) |>
  dplyr::arrange(date, time) |>
  dplyr::select(-county) |>
  dplyr::filter(mag %in% c("F2", "F3", "F4", "F5")) # note some transcription errors

OldDays.df <- OldTorn.sf |>
  sf::st_drop_geometry() |>
  dplyr::group_by(date) |>
  dplyr::reframe(nTor = dplyr::n(),
                 yr = lubridate::year(date))

df <- table(OldDays.df$nTor) |>
  data.frame()

Freq <- sort(df$Freq, decreasing = TRUE)
m_pl <- displ$new(Freq)
est <- estimate_xmin(m_pl)
#est$xmin = 1
m_pl$setXmin(est)

xmin <- c(xmin, m_pl$xmin)
w <- c(w, m_pl$pars)
}

w.df <- data.frame(Year = 1880:1989, w)

ggplot(data = w.df,
       mapping = aes(x = Year, y = w)) +
#  scale_y_continuous(limits = c(1, 2)) +
  scale_x_continuous(breaks = seq(1880, 1989, by = 10)) +
  geom_point() +
  geom_quantile()

summary(lm(w~Year, data = w.df))
```

